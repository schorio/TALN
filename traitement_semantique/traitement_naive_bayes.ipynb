{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2355f720",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import numpy as np # linear algebra\n",
    "import matplotlib.pyplot as plt # for data visualization purposes\n",
    "import seaborn as sns # for statistical data visualization\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972ff3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ''\n",
    "df = pd.read_csv(data, header=0, sep=',')\n",
    "# view dimensions of dataset\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64ddba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc99a7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621f8f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view summary of dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39653227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find categorical variables\n",
    "categorical = [var for var in df.columns if df[var].dtype=='O']\n",
    "print('There are {} categorical variables\\n'.format(len(categorical)))\n",
    "print('The categorical variables are :\\n\\n', categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050dea65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the categorical variables\n",
    "df[categorical].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3defacc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view frequency counts of values in categorical variables\n",
    "for var in categorical: \n",
    "    print(df[var].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9626bcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view frequency distribution of categorical variables\n",
    "for var in categorical: \n",
    "    print(df[var].value_counts()/np.float64(len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dd67ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check labels in classe variable\n",
    "df.classe.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d46f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check frequency distribution of values in classe variable\n",
    "df.classe.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959a0056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find numerical variables\n",
    "numerical = [var for var in df.columns if df[var].dtype!='O']\n",
    "print('There are {} numerical variables\\n'.format(len(numerical)))\n",
    "print('The numerical variables are :', numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0fe5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the numerical variables\n",
    "df[numerical].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ebe292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare feature vector and target variable \n",
    "X = df.drop(['classe'], axis=1)\n",
    "y = df['classe']\n",
    "\n",
    "#Split data into separate training and test set \n",
    "# split X and y into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "# check the shape of X_train and X_test\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f8f239",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e37e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6523cb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e63f70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3699028d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cols = X_train.columns\n",
    "cols = df.columns.drop(\"classe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede3e8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler = RobustScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_train = pd.DataFrame(X_train, columns=[cols])\n",
    "X_test = pd.DataFrame(X_test, columns=[cols])\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad6f5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a Gaussian Naive Bayes classifier on the training set\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# instantiate the model\n",
    "gnb = GaussianNB()\n",
    "\n",
    "\n",
    "# fit the model\n",
    "gnb.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62231956",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('', 'wb') as f:\n",
    "    pickle.dump(gnb, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0162107e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gnb.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008a8543",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print('Model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0161ba56",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = gnb.predict(X_train)\n",
    "y_pred_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996babdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print('Model accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8234ccdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for overfitting and underfitting\n",
    "# print the scores on training and test set\n",
    "\n",
    "print('Training set score: {:.4f}'.format(gnb.score(X_train, y_train)))\n",
    "\n",
    "print('Test set score: {:.4f}'.format(gnb.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "24605b8a",
   "metadata": {},
   "source": [
    "Compare model accuracy with null accuracy\n",
    "So, the model accuracy is 0.8083. But, we cannot say that our model is very good based on the above accuracy. We must compare it with the null accuracy. Null accuracy is the accuracy that could be achieved by always predicting the most frequent class.\n",
    "\n",
    "So, we should first check the class distribution in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9525ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check class distribution in test set\n",
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "49f95fb3",
   "metadata": {},
   "source": [
    "We can see that the occurences of most frequent class is 132. So, we can calculate null accuracy by dividing 132 by total number of occurences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a318423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check null accuracy score\n",
    "null_accuracy = (168/(168+129+101))\n",
    "print('Null accuracy score: {0:0.4f}'. format(null_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc75f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the Confusion Matrix and slice it into four pieces\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion matrix\\n\\n', cm)\n",
    "print('\\nTrue Positives(TP) = ', cm[0,0])\n",
    "print('\\nTrue Negatives(TN) = ', cm[1,1])\n",
    "print('\\nFalse Positives(FP) = ', cm[0,1])\n",
    "print('\\nFalse Negatives(FN) = ', cm[1,0])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "291e646c",
   "metadata": {},
   "source": [
    "Now, based on the above analysis we can conclude that our classification model accuracy is very good. Our model is doing a very good job in terms of predicting the class labels.\n",
    "\n",
    "But, it does not give the underlying distribution of values. Also, it does not tell anything about the type of errors our classifer is making.\n",
    "\n",
    "We have another tool called Confusion matrix that comes to our rescue.\n",
    "A confusion matrix is a tool for summarizing the performance of a classification algorithm. A confusion matrix will give us a clear picture of classification model performance and the types of errors produced by the model. It gives us a summary of correct and incorrect predictions broken down by each category. The summary is represented in a tabular form.\n",
    "\n",
    "Four types of outcomes are possible while evaluating a classification model performance. These four outcomes are described below:-\n",
    "\n",
    "True Positives (TP) – True Positives occur when we predict an observation belongs to a certain class and the observation actually belongs to that class.\n",
    "\n",
    "True Negatives (TN) – True Negatives occur when we predict an observation does not belong to a certain class and the observation actually does not belong to that class.\n",
    "\n",
    "False Positives (FP) – False Positives occur when we predict an observation belongs to a certain class but the observation actually does not belong to that class. This type of error is called Type I error.\n",
    "\n",
    "False Negatives (FN) – False Negatives occur when we predict an observation does not belong to a certain class but the observation actually belongs to that class. This is a very serious error and it is called Type II error.\n",
    "\n",
    "These four outcomes are summarized in a confusion matrix given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3886a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize confusion matrix with seaborn heatmap\n",
    "\n",
    "cm_matrix = pd.DataFrame(data=cm, columns=['Actual Positive:1', 'Actual Negative:0', 'None', 'None 1'], \n",
    "                                 index=['Predict Positive:1', 'Predict Negative:0', 'None', 'None 1'])\n",
    "\n",
    "sns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ba88e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5032aa32",
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = cm[0,0]\n",
    "TN = cm[1,1]\n",
    "FP = cm[0,1]\n",
    "FN = cm[1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf78de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print classification accuracy\n",
    "\n",
    "classification_accuracy = (TP + TN) / float(TP + TN + FP + FN)\n",
    "\n",
    "print('Classification accuracy : {0:0.4f}'.format(classification_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a267908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print classification error\n",
    "\n",
    "classification_error = (FP + FN) / float(TP + TN + FP + FN)\n",
    "\n",
    "print('Classification error : {0:0.4f}'.format(classification_error))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7778179d",
   "metadata": {},
   "source": [
    "Precision\n",
    "Precision can be defined as the percentage of correctly predicted positive outcomes out of all the predicted positive outcomes. It can be given as the ratio of true positives (TP) to the sum of true and false positives (TP + FP).\n",
    "\n",
    "So, Precision identifies the proportion of correctly predicted positive outcome. It is more concerned with the positive class than the negative class.\n",
    "\n",
    "Mathematically, precision can be defined as the ratio of TP to (TP + FP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f3e5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print precision score\n",
    "\n",
    "precision = TP / float(TP + FP)\n",
    "\n",
    "\n",
    "print('Precision : {0:0.4f}'.format(precision))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e496aea3",
   "metadata": {},
   "source": [
    "Recall\n",
    "Recall can be defined as the percentage of correctly predicted positive outcomes out of all the actual positive outcomes. It can be given as the ratio of true positives (TP) to the sum of true positives and false negatives (TP + FN). Recall is also called Sensitivity.\n",
    "\n",
    "Recall identifies the proportion of correctly predicted actual positives.\n",
    "\n",
    "Mathematically, recall can be given as the ratio of TP to (TP + FN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b68de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = TP / float(TP + FN)\n",
    "\n",
    "print('Recall or Sensitivity : {0:0.4f}'.format(recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f32a168",
   "metadata": {},
   "outputs": [],
   "source": [
    "#True Positive Rate. True Positive Rate is synonymous with Recall.\n",
    "true_positive_rate = TP / float(TP + FN)\n",
    "\n",
    "\n",
    "print('True Positive Rate : {0:0.4f}'.format(true_positive_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f16e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#False Positive Rate\n",
    "false_positive_rate = FP / float(FP + TN)\n",
    "\n",
    "\n",
    "print('False Positive Rate : {0:0.4f}'.format(false_positive_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0237d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specificity\n",
    "specificity = TN / (TN + FP)\n",
    "print('Specificity : {0:0.4f}'.format(specificity))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "daf5479f",
   "metadata": {},
   "source": [
    "f1-score\n",
    "f1-score is the weighted harmonic mean of precision and recall. The best possible f1-score would be 1.0 and the worst would be 0.0. f1-score is the harmonic mean of precision and recall. So, f1-score is always lower than accuracy measures as they embed precision and recall into their computation. The weighted average of f1-score should be used to compare classifier models, not global accuracy."
   ]
  },
  {
   "cell_type": "raw",
   "id": "4dbdfbe7",
   "metadata": {},
   "source": [
    "Support\n",
    "Support is the actual number of occurrences of the class in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e64e25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first 10 predicted probabilities of two classes- 0 and 1\n",
    "\n",
    "y_pred_prob = gnb.predict_proba(X_test)[0:10]\n",
    "\n",
    "y_pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5ea55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the probabilities in dataframe\n",
    "\n",
    "y_pred_prob_df = pd.DataFrame(data=y_pred_prob, columns=['Prob of - food', 'Prob of - sport', 'Prob of - art', 'Prob of - others'])\n",
    "y_pred_prob_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03590287",
   "metadata": {},
   "outputs": [],
   "source": [
    " #print the first 10 predicted probabilities for class 1 - Probability of >50K\n",
    "\n",
    "gnb.predict_proba(X_test)[0:10, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4641c305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the predicted probabilities for class 1 - Probability of >50K\n",
    "\n",
    "y_pred1 = gnb.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f999ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histogram of predicted probabilities\n",
    "\n",
    "\n",
    "# adjust the font size \n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "\n",
    "# plot histogram with 10 bins\n",
    "plt.hist(y_pred1, bins = 10)\n",
    "\n",
    "\n",
    "# set the title of predicted probabilities\n",
    "plt.title('Histogram of predicted probabilities of sport')\n",
    "\n",
    "\n",
    "# set the x-axis limit\n",
    "plt.xlim(0,1)\n",
    "\n",
    "\n",
    "# set the title\n",
    "plt.xlabel('Predicted probabilities of food')\n",
    "plt.ylabel('Frequency')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
